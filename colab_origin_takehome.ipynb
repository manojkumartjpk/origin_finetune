{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Origin Take-home: Prompted Segmentation (Structured Workflow)\n",
        "\n",
        "This notebook is organized to preserve *git-trackable* results at each stage:\n",
        "1. Dataset prep\n",
        "2. CLIPSeg zero-shot baseline\n",
        "3. SAM3 baseline (optional, if access/setup works)\n",
        "4. CLIPSeg fine-tuning\n",
        "5. Improvement experiments (threshold/epochs/image size/etc.)\n",
        "\n",
        "Use `results/` for small artifacts only (metrics JSON, notes, selected visuals).\n",
        "Do **not** commit datasets/checkpoints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 1) Mount Drive (optional)\n",
        "# Keep this False for faster training I/O. Use Drive mainly for persistence.\n",
        "USE_DRIVE = False\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 2) Clone / update repo\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_URL = \"https://github.com/<your-username>/origin_finetune.git\"  # @param {type:\"string\"}\n",
        "BRANCH = \"main\"  # @param {type:\"string\"}\n",
        "\n",
        "BASE_DIR = Path('/content')\n",
        "if USE_DRIVE:\n",
        "    BASE_DIR = Path('/content/drive/MyDrive/origin_takehome')\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "REPO_DIR = BASE_DIR / 'origin_finetune'\n",
        "if REPO_DIR.exists() and (REPO_DIR / '.git').exists():\n",
        "    %cd {REPO_DIR}\n",
        "    !git fetch origin\n",
        "    !git checkout {BRANCH}\n",
        "    !git pull --ff-only origin {BRANCH}\n",
        "else:\n",
        "    %cd {BASE_DIR}\n",
        "    !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n",
        "    %cd {REPO_DIR}\n",
        "\n",
        "print(\"Repo dir:\", REPO_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 3) Install dependencies\n",
        "%cd {REPO_DIR}\n",
        "!python -m pip install -q --upgrade pip\n",
        "!pip install -q -r requirements.txt roboflow requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 4) Create local artifact folders (tracked + untracked)\n",
        "%cd {REPO_DIR}\n",
        "!mkdir -p outputs/metrics outputs/eval_vis outputs/report_panels results/baselines results/finetuned results/experiments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 5) Enter Roboflow API key (hidden input)\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if 'ROBOFLOW_API_KEY' not in os.environ or not os.environ['ROBOFLOW_API_KEY']:\n",
        "    os.environ['ROBOFLOW_API_KEY'] = getpass('Enter ROBOFLOW_API_KEY: ')\n",
        "print('API key set:', bool(os.environ.get('ROBOFLOW_API_KEY')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 5b) (Optional) Enter Hugging Face token for SAM3 (hidden input)\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if 'HF_TOKEN' not in os.environ or not os.environ['HF_TOKEN']:\n",
        "    os.environ['HF_TOKEN'] = getpass('Enter HF_TOKEN (optional now, required for SAM3): ')\n",
        "print('HF token set:', bool(os.environ.get('HF_TOKEN')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 6) Download Roboflow datasets (forked versions)\n",
        "%cd {REPO_DIR}\n",
        "!mkdir -p data/raw\n",
        "\n",
        "!python -m src.data.download_roboflow     --api-key \"$ROBOFLOW_API_KEY\"     --workspace \"manojs-workspace-mbjw9\"     --project \"drywall-join-detect-jdsh1\"     --version 1     --format coco     --out-dir data/raw/drywall_join\n",
        "\n",
        "!python -m src.data.download_roboflow     --api-key \"$ROBOFLOW_API_KEY\"     --workspace \"manojs-workspace-mbjw9\"     --project \"cracks-3ii36-9iz5c\"     --version 1     --format coco     --out-dir data/raw/cracks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 7) Convert Roboflow COCO exports -> merged manifest -> label-aware resplit\n",
        "%cd {REPO_DIR}\n",
        "!mkdir -p data/processed\n",
        "\n",
        "!python -m src.data.prepare_from_roboflow_coco_export     --export-root data/raw/drywall_join     --dataset-tag drywall-join-detect     --out-dir data/processed\n",
        "\n",
        "!python -m src.data.prepare_from_roboflow_coco_export     --export-root data/raw/cracks     --dataset-tag cracks     --out-dir data/processed\n",
        "\n",
        "!python -m src.data.merge_manifests     --inputs data/processed/manifest_drywall-join-detect.csv data/processed/manifest_cracks.csv     --out data/processed/manifest_all.csv\n",
        "\n",
        "!python -m src.data.resplit_manifest     --manifest-csv data/processed/manifest_all.csv     --out data/processed/manifest_all_resplit.csv     --seed 42\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CLIPSeg Baseline (Zero-shot)\n",
        "Run this first and archive metrics to `results/baselines/` before any fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 8) Evaluate zero-shot CLIPSeg baseline\n",
        "%cd {REPO_DIR}\n",
        "!python -m src.eval_clipseg   --manifest-csv data/processed/manifest_all_resplit.csv   --model-dir CIDAS/clipseg-rd64-refined   --split test   --save-vis-dir outputs/eval_vis_clipseg_zeroshot   --max-vis 4   --metrics-out outputs/metrics/clipseg_zeroshot_test.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 9) Archive zero-shot CLIPSeg baseline artifacts into tracked results/\n",
        "%cd {REPO_DIR}\n",
        "!python scripts/archive_experiment.py   --category baselines   --run-id clipseg_zeroshot_v1   --summary-json outputs/metrics/clipseg_zeroshot_test.json   --copy outputs/eval_vis_clipseg_zeroshot   --notes \"Zero-shot CLIPSeg baseline on manifest_all_resplit test split\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SAM3 Baseline (Optional / Stretch)\n",
        "Use this section only if SAM3 access + setup works in Colab. Keep CLIPSeg zero-shot and fine-tuned results as the primary deliverables.\n",
        "\n",
        "Suggested policy:\n",
        "- First run a **subset** (e.g. 50-100 test images)\n",
        "- Save metrics in the same JSON schema as `src.eval_clipseg.py`\n",
        "- Archive to `results/baselines/sam3_zeroshot_*`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 10) Install/upgrade dependencies for SAM3 (run only if needed)\n",
        "%cd {REPO_DIR}\n",
        "!pip install -q -U transformers accelerate huggingface_hub\n",
        "# If the official sam3 package requires extra deps, install them here as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 11) Run SAM3 zero-shot baseline (subset first, then full)\n",
        "%cd {REPO_DIR}\n",
        "SAM3_RUN_ID = \"sam3_zeroshot_subset100_v1\"  # @param {type:\"string\"}\n",
        "SAM3_MODEL_ID = \"facebook/sam3\"  # @param {type:\"string\"}\n",
        "SAM3_MAX_SAMPLES = 100  # @param {type:\"integer\"}\n",
        "\n",
        "# Edit the command strings below if you change the values above.\n",
        "!python -m src.eval_sam3   --manifest-csv data/processed/manifest_all_resplit.csv   --model-id \"facebook/sam3\"   --split test   --max-samples 100   --save-vis-dir outputs/eval_vis_sam3_zeroshot_subset100_v1   --max-vis 4   --metrics-out outputs/metrics/sam3_zeroshot_subset100_v1.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 12) Archive SAM3 baseline (run after SAM3 metrics exist)\n",
        "%cd {REPO_DIR}\n",
        "SAM3_RUN_ID = \"sam3_zeroshot_subset100_v1\"  # @param {type:\"string\"}\n",
        "# Edit the command strings below if you change the run id above.\n",
        "!python scripts/archive_experiment.py   --category baselines   --run-id sam3_zeroshot_subset100_v1   --summary-json outputs/metrics/sam3_zeroshot_subset100_v1.json   --copy outputs/eval_vis_sam3_zeroshot_subset100_v1   --notes \"SAM3 zero-shot baseline (subset/full; check max_samples in metrics JSON)\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CLIPSeg Fine-tuning (Main Result)\n",
        "Use unique output directories per experiment to avoid overwriting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 13) Fine-tune CLIPSeg (main run)\n",
        "%cd {REPO_DIR}\n",
        "MAIN_RUN_ID = \"clipseg_ft_e8_352_v1\"  # @param {type:\"string\"}\n",
        "FT_OUTPUT_DIR = f\"checkpoints/{MAIN_RUN_ID}\"\n",
        "print(\"FT_OUTPUT_DIR =\", FT_OUTPUT_DIR)\n",
        "\n",
        "# Edit the command strings below if you change MAIN_RUN_ID.\n",
        "!python -m src.train_clipseg     --manifest-csv data/processed/manifest_all_resplit.csv     --output-dir checkpoints/clipseg_ft_e8_352_v1     --epochs 8     --batch-size 4     --image-size 352     --lr 2e-5     --grad-accum-steps 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 14) Evaluate fine-tuned CLIPSeg (main run)\n",
        "%cd {REPO_DIR}\n",
        "MAIN_RUN_ID = \"clipseg_ft_e8_352_v1\"  # must match previous cell if rerun separately\n",
        "FT_OUTPUT_DIR = f\"checkpoints/{MAIN_RUN_ID}\"\n",
        "FT_EVAL_VIS_DIR = f\"outputs/eval_vis_{MAIN_RUN_ID}\"\n",
        "FT_METRICS_OUT = f\"outputs/metrics/{MAIN_RUN_ID}_test.json\"\n",
        "\n",
        "# Edit the command strings below if you change MAIN_RUN_ID.\n",
        "!python -m src.eval_clipseg   --manifest-csv data/processed/manifest_all_resplit.csv   --model-dir checkpoints/clipseg_ft_e8_352_v1   --split test   --save-vis-dir outputs/eval_vis_clipseg_ft_e8_352_v1   --max-vis 4   --metrics-out outputs/metrics/clipseg_ft_e8_352_v1_test.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 15) Create side-by-side report panels (orig | GT | pred) for main run\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "repo = REPO_DIR\n",
        "MAIN_RUN_ID = \"clipseg_ft_e8_352_v1\"  # @param {type:\"string\"}\n",
        "manifest = pd.read_csv(repo / 'data/processed/manifest_all_resplit.csv')\n",
        "test_df = manifest[manifest['split'] == 'test'].copy()\n",
        "pred_dir = repo / 'outputs' / f'eval_vis_{MAIN_RUN_ID}'\n",
        "panel_dir = repo / 'outputs' / f'report_panels_{MAIN_RUN_ID}'\n",
        "panel_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "saved = 0\n",
        "for _, row in test_df.iterrows():\n",
        "    image_id = str(row['image_id'])\n",
        "    pred_path = pred_dir / f\"{image_id}__pred.png\"\n",
        "    gt_path = pred_dir / f\"{image_id}__gt.png\"\n",
        "    if not pred_path.exists() or not gt_path.exists():\n",
        "        continue\n",
        "    img = Image.open(row['image_path']).convert('RGB')\n",
        "    gt = Image.open(gt_path).convert('L')\n",
        "    pred = Image.open(pred_path).convert('L')\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    axes[0].imshow(img); axes[0].set_title('Original'); axes[0].axis('off')\n",
        "    axes[1].imshow(gt, cmap='gray'); axes[1].set_title('GT'); axes[1].axis('off')\n",
        "    axes[2].imshow(pred, cmap='gray'); axes[2].set_title('Pred'); axes[2].axis('off')\n",
        "    fig.suptitle(f\"{image_id} | {row['label']}\")\n",
        "    out = panel_dir / f\"{image_id}__panel.png\"\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out, dpi=150, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    print(\"saved\", out)\n",
        "    saved += 1\n",
        "    if saved >= 4:\n",
        "        break\n",
        "print(\"panels saved:\", saved)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 15b) Create report panels (4 per dataset) for main run [recommended]\n",
        "%cd {REPO_DIR}\n",
        "MAIN_RUN_ID = \"clipseg_ft_e8_352_v1\"  # @param {type:\"string\"}\n",
        "PANELS_PER_DATASET = 4  # @param {type:\"integer\"}\n",
        "\n",
        "!python scripts/make_report_panels.py   --manifest-csv data/processed/manifest_all_resplit.csv   --pred-dir outputs/eval_vis_{MAIN_RUN_ID}   --out-dir outputs/report_panels_{MAIN_RUN_ID}   --split test   --group-col dataset_tag   --per-group {PANELS_PER_DATASET}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 16) Model size (runtime & footprint section) for main run\n",
        "%cd {REPO_DIR}\n",
        "MAIN_RUN_ID = \"clipseg_ft_e8_352_v1\"  # @param {type:\"string\"}\n",
        "!du -sh checkpoints/clipseg_ft_e8_352_v1\n",
        "!du -sh checkpoints/clipseg_ft_e8_352_v1/*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 17) Archive fine-tuned CLIPSeg main run artifacts into tracked results/\n",
        "%cd {REPO_DIR}\n",
        "MAIN_RUN_ID = \"clipseg_ft_e8_352_v1\"  # @param {type:\"string\"}\n",
        "# Edit the command strings below if you change MAIN_RUN_ID.\n",
        "!python scripts/archive_experiment.py   --category finetuned   --run-id clipseg_ft_e8_352_v1   --summary-json outputs/metrics/clipseg_ft_e8_352_v1_test.json   --copy checkpoints/clipseg_ft_e8_352_v1/best_metrics.json checkpoints/clipseg_ft_e8_352_v1/train_history.json outputs/report_panels_clipseg_ft_e8_352_v1   --notes \"Main fine-tuned CLIPSeg run (full fine-tuning, 8 epochs, image_size=352)\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Improvement Experiments (run one at a time, archive each)\n",
        "Recommended order:\n",
        "1. Threshold sweep (cheap)\n",
        "2. More epochs (12/16)\n",
        "3. Larger image size (512, smaller batch)\n",
        "4. Class balancing / weighting\n",
        "5. Prompt augmentation expansion\n",
        "6. Post-processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 18) Threshold sweep helper on main checkpoint (cheap improvement test)\n",
        "%cd {REPO_DIR}\n",
        "MAIN_RUN_ID = \"clipseg_ft_e8_352_v1\"  # @param {type:\"string\"}\n",
        "for thr in [0.3, 0.4, 0.5, 0.6]:\n",
        "    out_json = f\"outputs/metrics/{MAIN_RUN_ID}_test_thr{str(thr).replace('.', '')}.json\"\n",
        "    print(\"\\n=== threshold\", thr, \"===\")\n",
        "    # If interpolation fails in your Colab, replace {thr}/{MAIN_RUN_ID} with literal values.\n",
        "    !python -m src.eval_clipseg       --manifest-csv data/processed/manifest_all_resplit.csv       --model-dir checkpoints/{MAIN_RUN_ID}       --split test       --threshold {thr}       --metrics-out {out_json}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 19) Custom CLIPSeg experiment template (new checkpoint dir each time)\n",
        "%cd {REPO_DIR}\n",
        "EXP_RUN_ID = \"clipseg_ft_e12_352_trial1\"  # @param {type:\"string\"}\n",
        "EXP_EPOCHS = 12  # @param {type:\"integer\"}\n",
        "EXP_IMAGE_SIZE = 352  # @param {type:\"integer\"}\n",
        "EXP_BATCH = 4  # @param {type:\"integer\"}\n",
        "EXP_LR = 2e-5  # @param {type:\"number\"}\n",
        "\n",
        "# If shell interpolation fails in your Colab, replace {EXP_*} placeholders with literals.\n",
        "!python -m src.train_clipseg     --manifest-csv data/processed/manifest_all_resplit.csv     --output-dir checkpoints/{EXP_RUN_ID}     --epochs {EXP_EPOCHS}     --batch-size {EXP_BATCH}     --image-size {EXP_IMAGE_SIZE}     --lr {EXP_LR}\n",
        "\n",
        "!python -m src.eval_clipseg     --manifest-csv data/processed/manifest_all_resplit.csv     --model-dir checkpoints/{EXP_RUN_ID}     --split test     --metrics-out outputs/metrics/{EXP_RUN_ID}_test.json\n",
        "\n",
        "!python scripts/archive_experiment.py   --category experiments   --run-id {EXP_RUN_ID}   --summary-json outputs/metrics/{EXP_RUN_ID}_test.json   --copy checkpoints/{EXP_RUN_ID}/best_metrics.json checkpoints/{EXP_RUN_ID}/train_history.json   --notes \"Custom experiment; record changes in run_id and report notes\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 19b) Create balanced-train manifest helper (equal images from both datasets for train split)\n",
        "%cd {REPO_DIR}\n",
        "BAL_MANIFEST = \"data/processed/manifest_all_resplit_balanced_train_by_dataset.csv\"  # @param {type:\"string\"}\n",
        "\n",
        "!python -m src.data.balance_manifest_by_group   --manifest-csv data/processed/manifest_all_resplit.csv   --out {BAL_MANIFEST}   --split train   --group-col dataset_tag   --unit-col image_id   --seed 42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 19c) Planned Run 1: rd64-refined | 16 epochs | 512x512 | threshold 0.4\n",
        "%cd {REPO_DIR}\n",
        "RUN_ID = \"clipseg_ft_e16_512_thr04_v1\"  # @param {type:\"string\"}\n",
        "\n",
        "# Notes:\n",
        "# - threshold here affects validation metrics/checkpoint selection during training\n",
        "# - optimized for A100 throughput; reduce batch if you hit OOM\n",
        "!python -m src.train_clipseg   --manifest-csv data/processed/manifest_all_resplit.csv   --model-name CIDAS/clipseg-rd64-refined   --output-dir checkpoints/{RUN_ID}   --epochs 16   --batch-size 8   --grad-accum-steps 1   --image-size 512   --lr 2e-5   --threshold 0.4   --num-workers 8   --persistent-workers   --prefetch-factor 4   --tf32   --amp-dtype bf16   --no-processor-resize\n",
        "\n",
        "!python -m src.eval_clipseg   --manifest-csv data/processed/manifest_all_resplit.csv   --model-dir checkpoints/{RUN_ID}   --split test   --image-size 512   --threshold 0.4   --num-workers 8   --persistent-workers   --prefetch-factor 4   --no-processor-resize   --metrics-out outputs/metrics/{RUN_ID}_test.json\n",
        "\n",
        "!python scripts/archive_experiment.py   --category experiments   --run-id {RUN_ID}   --summary-json outputs/metrics/{RUN_ID}_test.json   --copy checkpoints/{RUN_ID}/best_metrics.json checkpoints/{RUN_ID}/train_history.json   --notes \"Run1: rd64-refined, 16 epochs, image_size=512, threshold=0.4 (val/test eval threshold).\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 19d) Planned Run 2: rd64-refined | 4 epochs | 1024x1024 | threshold 0.4 (high-risk/OOM)\n",
        "%cd {REPO_DIR}\n",
        "RUN_ID = \"clipseg_ft_e4_1024_thr04_v1\"  # @param {type:\"string\"}\n",
        "\n",
        "# 1024x1024 remains memory-heavy. On A100 start at batch_size=2; if OOM, drop to 1.\n",
        "!python -m src.train_clipseg   --manifest-csv data/processed/manifest_all_resplit.csv   --model-name CIDAS/clipseg-rd64-refined   --output-dir checkpoints/{RUN_ID}   --epochs 4   --batch-size 2   --grad-accum-steps 1   --image-size 1024   --lr 2e-5   --threshold 0.4   --num-workers 8   --persistent-workers   --prefetch-factor 4   --tf32   --amp-dtype bf16   --no-processor-resize\n",
        "\n",
        "!python -m src.eval_clipseg   --manifest-csv data/processed/manifest_all_resplit.csv   --model-dir checkpoints/{RUN_ID}   --split test   --batch-size 2   --image-size 1024   --threshold 0.4   --num-workers 8   --persistent-workers   --prefetch-factor 4   --no-processor-resize   --metrics-out outputs/metrics/{RUN_ID}_test.json\n",
        "\n",
        "!python scripts/archive_experiment.py   --category experiments   --run-id {RUN_ID}   --summary-json outputs/metrics/{RUN_ID}_test.json   --copy checkpoints/{RUN_ID}/best_metrics.json checkpoints/{RUN_ID}/train_history.json   --notes \"Run2: rd64-refined, 4 epochs, image_size=1024, threshold=0.4. A100-tuned defaults (batch_size=2, grad_accum=1); reduce batch if OOM.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 19e) Planned Run 3: rd64-refined | 8 epochs | 512x512 | threshold 0.4 | balanced train by dataset\n",
        "%cd {REPO_DIR}\n",
        "RUN_ID = \"clipseg_ft_e8_512_thr04_baltrain_dataset_v1\"  # @param {type:\"string\"}\n",
        "BAL_MANIFEST = \"data/processed/manifest_all_resplit_balanced_train_by_dataset.csv\"  # @param {type:\"string\"}\n",
        "\n",
        "# Make balanced manifest first (cell 19b), then run this cell.\n",
        "!python -m src.train_clipseg   --manifest-csv {BAL_MANIFEST}   --model-name CIDAS/clipseg-rd64-refined   --output-dir checkpoints/{RUN_ID}   --epochs 8   --batch-size 8   --grad-accum-steps 1   --image-size 512   --lr 2e-5   --threshold 0.4   --num-workers 8   --persistent-workers   --prefetch-factor 4   --tf32   --amp-dtype bf16   --no-processor-resize\n",
        "\n",
        "!python -m src.eval_clipseg   --manifest-csv data/processed/manifest_all_resplit.csv   --model-dir checkpoints/{RUN_ID}   --split test   --image-size 512   --threshold 0.4   --num-workers 8   --persistent-workers   --prefetch-factor 4   --no-processor-resize   --metrics-out outputs/metrics/{RUN_ID}_test.json\n",
        "\n",
        "!python scripts/archive_experiment.py   --category experiments   --run-id {RUN_ID}   --summary-json outputs/metrics/{RUN_ID}_test.json   --copy checkpoints/{RUN_ID}/best_metrics.json checkpoints/{RUN_ID}/train_history.json   --notes \"Run3: rd64-refined, 8 epochs, image_size=512, threshold=0.4, train split balanced by dataset_tag (equal image_id count per dataset). Eval on full test manifest.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 20) Example inference for required output mask naming (main run)\n",
        "%cd {REPO_DIR}\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "MAIN_RUN_ID = \"clipseg_ft_e8_352_v1\"\n",
        "m = pd.read_csv('data/processed/manifest_all_resplit.csv')\n",
        "row = m.iloc[0]\n",
        "print('Example image path:', row['image_path'])\n",
        "print('Example label:', row['label'])\n",
        "\n",
        "# Uncomment one:\n",
        "# !python -m src.infer_clipseg --model-dir checkpoints/{MAIN_RUN_ID} --image \"{row['image_path']}\" --prompt \"segment crack\" --out-dir outputs/pred_masks\n",
        "# !python -m src.infer_clipseg --model-dir checkpoints/{MAIN_RUN_ID} --image \"{row['image_path']}\" --prompt \"segment taping area\" --out-dir outputs/pred_masks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 21) Inspect tracked result artifacts before committing\n",
        "%cd {REPO_DIR}\n",
        "!find results -maxdepth 4 -type f | sort\n",
        "!git status --short\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Commit/Pull/Push from Colab\n",
        "If you want to push result artifacts from Colab, authenticate carefully (PAT/token). Prefer committing only notebook/code/`results/` files.\n",
        "\n",
        "If you prefer safer workflow:\n",
        "- finish runs in Colab\n",
        "- copy/pull changed repo files locally\n",
        "- commit/push from local machine\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submission Checklist\n",
        "- GitHub link (codebase)\n",
        "- Colab link (shareable)\n",
        "- PDF report with:\n",
        "  - Methodology\n",
        "  - Data-preparation\n",
        "  - Results (metrics table + visuals + runtime/footprint)\n",
        "  - Failure cases + potential solutions\n",
        "\n",
        "Security note: rotate/revoke your Roboflow API key after submission.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "origin_takehome_clipseg.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}